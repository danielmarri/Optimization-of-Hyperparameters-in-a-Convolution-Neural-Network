{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using Statistical Models in Hyperparameter Optimization</center>\n",
    "\n",
    "<center>by Cecilie Dura Andr√©</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Statistical Models are used multiple times in this project.\n",
    "\n",
    "Bayesian optimization (BO) is used and it uses statistical modeling. BO is used to maximize the \"black box\" functions, this could be Machine Learning or Deep Learning algorithms. BO takes the algorithm and tests it on a sequence of test points to determine the optimal values for hyperparameters. This is done by approximations since we cannot solve the problem analytically. The approximation is a surrogate model (a probabilistic model) based on the results with the associated hyperparameter. The probabilistic model is a probability distribution and it describes the probability of a random phenomenon. The probabilistic model is fitted with a normal distribution described by a posterior mean and posterior standard deviation/uncertainty (Ravikumar 2018). \n",
    "\n",
    "An acquisition function is then used to look at different trade-offs of picking known maxima and explore uncertain locations in the hyperparameter space. These steps are iterated, thus we should get a better and better approximation until the maximum number of iterations is met. The best results can then be found with the respective hyperparameter(s) (Ravikumar 2018). \n",
    "\n",
    "\n",
    "An example can be seen here:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*PhKGj_bZlND8IEfII426wA.png\" width=\"50%\">\n",
    "\n",
    "<p style=\"text-align: right;\">Image from: https://towardsdatascience.com/shallow-understanding-on-bayesian-optimization-324b6c1f7083 </p>\n",
    "\n",
    "\n",
    "Statistical models is also used to describe the results of CNN with and without hyperparameter optimization (HO). CNN with and without HO was trained respectively 10 times each. For each of these 10 times was the accuracy calculated for each epoch for the training, validation, and test set. Thus, the mean accuracy and standard deviation could be calculated respectively for the training, validation, and test set for each epoch. \n",
    "\n",
    "\n",
    "\n",
    "This could be: Learning rate, momentum, weight decay, epsilon, and rho - even the numper of epochs can be optimized. In our case we only used BO to find the learning rate because it was faster than finding more hyperparameters. This was done ten times and the mean and standard deviation of the accuracy for each epoch was plotted. It was compared to the accuracy for running the CNN withouth hyperparameter optimization (HO) over ten times. Here the mean and standard deviation of the accuracy for each epoch where also calculated. The results can be seen here below. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "Meghana Ravikumar (2018): \" Let's talk Bayesian Optimization\". URL: https://mlconf.com/blog/lets-talk-bayesian-optimization/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
