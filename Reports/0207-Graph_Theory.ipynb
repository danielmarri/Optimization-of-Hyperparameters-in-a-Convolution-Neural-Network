{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using Graph Theory in Deep Learning and Optimization</center>\n",
    "\n",
    "<center>by Cecilie Dura André</center>\n",
    "<center>andrecec@msu.edu</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph theory is a branch of mathematics and it consists of ways to structure data and show them. A graph consists of nodes (a unit that holds information) and edges that connect each of the nodes and can also hold information. This information in the nodes could be cities and the edges could be the length in between the cities or coordinate differences. Thus, it can be used to find the shortest driving route. Other information can also be stored in the nodes and edges, such as link structure of a website, and the modeling of computer networks. Therefore, graph theory can be used for data that is non-euclidean and euclidean (O’Regan 2013).\n",
    "\n",
    "In Graph theory graphs represent data interconnected in some way. The graphs can be distinguished between undirected graphs and directed graphs. Here, they represent respectively graphs that have mathematically equivalent symmetric relations or having binary relations (O’Regan 2013).\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2000/1*01whFGyfYCasLLqZAVUwYg.png\" width=\"50%\">\n",
    "\n",
    "<p style=\"text-align: right;\">Image from: https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b </p>\n",
    "\n",
    "Then the graph can either have heterogeneous or homogeneous nodes. Also, the graphs can be static and dynamic. This means the nodes and edges do not change or they change, can be added, deleted, etc. Other terms, which can be used to describe graphs can be planar and non-planer. This, respectively tells if the edges cross or do not (Flawnson 2019).\n",
    "\n",
    "In Deep Learning, Neural Networks (NN) are often used. NN are structured perceptrons. Perceptrons have an output, which can be described as  $f(y=mx+b)$. Here, $x$  is the input to the perceptron, which is multiplied with a weight,  $w$, and a bias, $b$, is added in the end to get the output (Flawnson 2019).\n",
    " \n",
    "<img src=\"https://miro.medium.com/max/3200/0*DCsw_xQU0VQrIZP4\" width=\"80%\">\n",
    "<p style=\"text-align: right;\">Image from: https://miro.medium.com/max/3200/0*uR4mNZ6pYT1L7COX </p>\n",
    "\n",
    "When the perceptrons are structured together a NN is created. Each of the perceptrons passed information to the next perceptrons and in each layer, the input is multiplied with a weight and a bias is added to it. In the end, the network predicts a label, which it is trained against. The networks of perceptrons are trained by minimizing an error. The error is the predicted outcome compared to the expected outcome. This error is minimized by adjusting the weights, which is done by a process called backpropagation (Flawnson 2019).\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1582/0*JwcquDlAfzhE-y1X\" width=\"80%\">\n",
    "<p style=\"text-align: right;\">Image from: https://miro.medium.com/max/3200/0*uR4mNZ6pYT1L7COX </p>\n",
    "\n",
    "From the above-mentioned theory, NNs can be seen as a special group of graphs and they resemble what is called multipartite graphs. They have the same structure and concepts (Flawnson 2019). Thus, in my project, I am working on a way to optimize the training of a \"graph\" - and in a way, my model is a graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "Flawnson, Tong. 2019. “Everything You Need to Know about Graph Theory for Deep Learning.” Toward Data Science. April 23. https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "cite2c": {
   "citations": {
    "7514239/4AFCQBNZ": {
     "ISBN": "978-1-4471-4533-2 978-1-4471-4534-9",
     "URL": "http://link.springer.com/10.1007/978-1-4471-4534-9",
     "accessed": {
      "day": 6,
      "month": 2,
      "year": 2020
     },
     "author": [
      {
       "family": "O’Regan",
       "given": "Gerard"
      }
     ],
     "event-place": "London",
     "id": "7514239/4AFCQBNZ",
     "issued": {
      "year": 2013
     },
     "language": "en",
     "note": "DOI: 10.1007/978-1-4471-4534-9",
     "publisher": "Springer London",
     "publisher-place": "London",
     "title": "Mathematics in Computing",
     "type": "book"
    },
    "7514239/9CSH3CZM": {
     "URL": "https://towardsdatascience.com/graph-theory-and-deep-learning-know-hows-6556b0e9891b",
     "author": [
      {
       "family": "Flawnson",
       "given": "Tong"
      }
     ],
     "genre": "Toward Data Science",
     "id": "7514239/9CSH3CZM",
     "issued": {
      "day": 23,
      "month": 4,
      "year": 2019
     },
     "title": "Everything you need to know about Graph Theory for Deep Learning",
     "type": "webpage"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
