{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using Optimization in Hyperparameter settings in Deep Learning</center>\n",
    "\n",
    "<center>by Cecilie Dura André</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://blog.ml.cmu.edu/wp-content/uploads/2018/12/heatmap.001-min.jpeg\" width=\"90%\">\n",
    "<p style=\"text-align: right;\">Image from: https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Deep Learning people often has to explore network structure, regularization, and optimization to get the best model. Thus, automated hyperparameter optimization (HPO) is needed and it is shown that tailed solution to a problem leads to the state-of-the-art performance of the model (Feurer and Hutter 2019). It also leads to fair comparisons of the model with different hyperparameters, thus the reproducibility of the studies would become better (J Bergstra, Yamins, and Cox).\n",
    "\n",
    "The most basic HPO method is called Grid search or full factorial design. Here the user selects a couple of given values for each of the hyperparameters. Then, a grid search can be used to evaluate the Cartesian product. This requires a lot of computational memory and this model also suffers from the curse of dimensionality, when the number of hyperparameters becomes too big (“Design and Analysis of Experiments by Douglas Montgomery: A Supplement for Using JMP” 2013).\n",
    "\n",
    "Alternatively to grid search there is also something called random search (James Bergstra and Bengio). Here a selected finite search space is given and the model can randomly select values for the hyperparameters within the search space. This method is preferred over grid search if one of the hyperparameters are more important than others (Hutter, Hoos, and Leyton-Brown). This method will often with time fine the optimum, but it takes a longer time than guided search methods.\n",
    "\n",
    "Population-based methods, e.g. genetic algorithms, evolutionary algorithms, and evolutionary strategies, can also be used. Here a set of configurations is maintained. Small changes and different combinations are used to find a better configuration.\n",
    "\n",
    "Bayesian optimization is the preferred method for HPO in tuning deep neural networks. By using Bayesian optimization in deep learning state-of-the-art results have been seen in image classification (Snoek, Larochelle, and Adams), (Snoek et al. 2015). Bayesian optimization is an iterative model, which first calculates the probabilistic surrogate model fitted to all observations. An acquisition function then determines different points, trade-offs, e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "Bergstra, James, and Yoshua Bengio. “Random Search for Hyper-Parameter Optimization,” 25.\n",
    "\n",
    "Bergstra, J, D Yamins, and D D Cox. “Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures,” 9. “Design and Analysis of Experiments by Douglas Montgomery: A Supplement for Using JMP.” 2013, 26.\n",
    "\n",
    "Feurer, Matthias, and Frank Hutter. 2019. “Hyperparameter Optimization.” In Automated Machine Learning, edited by Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren, 3–33. Cham: Springer International Publishing. http://link.springer.com/10.1007/978-3-030-05318-5_1.\n",
    "\n",
    "Hutter, Frank, Holger Hoos, and Kevin Leyton-Brown. “An Efficient Approach for Assessing Hyperparameter Importance,” 9.\n",
    "\n",
    "Snoek, Jasper, Hugo Larochelle, and Ryan P Adams. “Practical Bayesian Optimization of Machine Learning Algorithms,” 9.\n",
    "\n",
    "Snoek, Jasper, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md Mostofa Ali Patwary, \n",
    "\n",
    "Prabhat, and Ryan P. Adams. 2015. “Scalable Bayesian Optimization Using Deep Neural Networks.” arXiv:1502.05700 [stat], July. http://arxiv.org/abs/1502.05700."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "cite2c": {
   "citations": {
    "7514239/36DWBDKM": {
     "abstract": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efﬁcient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to conﬁgure neural networks and deep belief networks. Compared with neural networks conﬁgured by a pure grid search, we ﬁnd that random search over the same domain is able to ﬁnd models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search ﬁnds better models by effectively searching a larger, less promising conﬁguration space. Compared with deep belief networks conﬁgured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional conﬁguration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for conﬁguring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.",
     "author": [
      {
       "family": "Bergstra",
       "given": "James"
      },
      {
       "family": "Bengio",
       "given": "Yoshua"
      }
     ],
     "id": "7514239/36DWBDKM",
     "language": "en",
     "page": "25",
     "page-first": "25",
     "title": "Random Search for Hyper-Parameter Optimization",
     "type": "article-journal"
    },
    "7514239/KG7DCJK2": {
     "id": "7514239/KG7DCJK2",
     "title": "Feurer og Hutter - 2019 - Hyperparameter Optimization.pdf",
     "type": "article"
    },
    "7514239/MSN9YR7Y": {
     "URL": "http://arxiv.org/abs/1502.05700",
     "abstract": "Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions deﬁned by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically ﬁt using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization.",
     "accessed": {
      "day": 27,
      "month": 1,
      "year": 2020
     },
     "author": [
      {
       "family": "Snoek",
       "given": "Jasper"
      },
      {
       "family": "Rippel",
       "given": "Oren"
      },
      {
       "family": "Swersky",
       "given": "Kevin"
      },
      {
       "family": "Kiros",
       "given": "Ryan"
      },
      {
       "family": "Satish",
       "given": "Nadathur"
      },
      {
       "family": "Sundaram",
       "given": "Narayanan"
      },
      {
       "family": "Patwary",
       "given": "Md Mostofa Ali"
      },
      {
       "family": "Prabhat",
       "given": ""
      },
      {
       "family": "Adams",
       "given": "Ryan P."
      }
     ],
     "container-title": "arXiv:1502.05700 [stat]",
     "id": "7514239/MSN9YR7Y",
     "issued": {
      "day": 13,
      "month": 7,
      "year": 2015
     },
     "language": "en",
     "note": "arXiv: 1502.05700",
     "title": "Scalable Bayesian Optimization Using Deep Neural Networks",
     "type": "article-journal"
    },
    "7514239/Q2WCVFJM": {
     "ISBN": "978-3-030-05317-8 978-3-030-05318-5",
     "URL": "http://link.springer.com/10.1007/978-3-030-05318-5_1",
     "abstract": "Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We ﬁrst discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-ﬁdelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.",
     "accessed": {
      "day": 26,
      "month": 1,
      "year": 2020
     },
     "author": [
      {
       "family": "Feurer",
       "given": "Matthias"
      },
      {
       "family": "Hutter",
       "given": "Frank"
      }
     ],
     "container-title": "Automated Machine Learning",
     "editor": [
      {
       "family": "Hutter",
       "given": "Frank"
      },
      {
       "family": "Kotthoff",
       "given": "Lars"
      },
      {
       "family": "Vanschoren",
       "given": "Joaquin"
      }
     ],
     "event-place": "Cham",
     "id": "7514239/Q2WCVFJM",
     "issued": {
      "year": 2019
     },
     "language": "en",
     "note": "DOI: 10.1007/978-3-030-05318-5_1",
     "page": "3-33",
     "page-first": "3",
     "publisher": "Springer International Publishing",
     "publisher-place": "Cham",
     "title": "Hyperparameter Optimization",
     "type": "chapter"
    },
    "7514239/SEZPLKZK": {
     "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",
     "author": [
      {
       "family": "Snoek",
       "given": "Jasper"
      },
      {
       "family": "Larochelle",
       "given": "Hugo"
      },
      {
       "family": "Adams",
       "given": "Ryan P"
      }
     ],
     "id": "7514239/SEZPLKZK",
     "language": "en",
     "page": "9",
     "page-first": "9",
     "title": "Practical Bayesian Optimization of Machine Learning Algorithms",
     "type": "article-journal"
    },
    "7514239/VJC7RSXT": {
     "abstract": "The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efﬁcient methods that can be used to gain such insight, leveraging random forest models ﬁt on the data already gathered by Bayesian optimization. We ﬁrst introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very highdimensional cases—most performance variation is attributable to just a few hyperparameters.",
     "author": [
      {
       "family": "Hutter",
       "given": "Frank"
      },
      {
       "family": "Hoos",
       "given": "Holger"
      },
      {
       "family": "Leyton-Brown",
       "given": "Kevin"
      }
     ],
     "id": "7514239/VJC7RSXT",
     "language": "en",
     "page": "9",
     "page-first": "9",
     "title": "An Efficient Approach for Assessing Hyperparameter Importance",
     "type": "article-journal"
    },
    "7514239/YJRV46MX": {
     "abstract": "Many computer vision algorithms depend on conﬁguration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method’s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes diﬃcult to know whether a given technique is genuinely better, or simply better tuned.",
     "author": [
      {
       "family": "Bergstra",
       "given": "J"
      },
      {
       "family": "Yamins",
       "given": "D"
      },
      {
       "family": "Cox",
       "given": "D D"
      }
     ],
     "id": "7514239/YJRV46MX",
     "language": "en",
     "page": "9",
     "page-first": "9",
     "title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures",
     "type": "article-journal"
    },
    "7514239/ZDHQEBIV": {
     "id": "7514239/ZDHQEBIV",
     "issued": {
      "year": 2013
     },
     "language": "en",
     "page": "26",
     "page-first": "26",
     "title": "Design and Analysis of Experiments by Douglas Montgomery: A supplement for Using JMP",
     "type": "article-journal"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
