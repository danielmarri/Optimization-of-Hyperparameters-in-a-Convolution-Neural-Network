{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Using Ordinary Differential Equations in Hyperparameter Optimization of Neural Networks </center>\n",
    "\n",
    "<center>by Cecilie Dura Andre</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ordinary differential equations can be used in training Neural Networks. The process of training a neural network is called forward and back-propagation and the main idea uses gradients. To find the optimal value that minimizes the gradients ordinary differential equations (ODEs) can be used and they are solved numerical (Tamburro 2019).\n",
    "\n",
    "As described in the other reports, neural networks consist of perceptrons in layers and they \"learn\" by adjusting the weights. First data is propagated forward through the network and transformed through the different layers, $l$. Thus, a given layer gets input from a previous layer, $A^{l-1}$, and it is linearly transformed with weights, $W^l$, and the bias, $b^l$, of the layer is added in the end. Thus the output of the perceptron for one layer would be(Tamburro 2019):\n",
    "\n",
    "<center>$Z^l=W^l A^{l-1}+b^l$</center>\n",
    "\n",
    "Then a non-linear transformation, an activation function, to get the input for the next layer:\n",
    "\n",
    "\n",
    "<center>$A^l=g^l (Z^l)$</center>\n",
    "\n",
    "And this continue until the end. Then the cost, $C$, is calculated and the cost is used in the backprogation (Tamburro 2019). Thus, the cost with resepect to the weights can for each layer be calculated by: \n",
    "\n",
    "\n",
    "<center>$ \\frac{dC}{dW^l}=\\frac{dC}{dZ^l} (A^{l-1})^T$</center>\n",
    "\n",
    "Here:\n",
    "\n",
    "<center>$ \\frac{dC}{dZ^l}=(W^{l+1})^T  \\cdot \\frac{dC}{dZ^{l+1}} \\cdot g^{l'}(Z^l)= dA^l \\cdot g^{l'}(Z^l) $</center>\n",
    "\n",
    "\n",
    "Thus they can be written as:\n",
    "\n",
    "<center>$ \\frac{dC}{dW}=f(W)$</center>\n",
    "\n",
    "This ODE can be solved numerical with a small step size of the intragtor (Tamburro 2019). This will change the ODE to:\n",
    "\n",
    "<center>$ \\frac{dC}{dW(t)}=f(W,t)$</center>\n",
    "\n",
    "This way of optimizing the neural networks require no hyperparameter tuning and it gets the same results as other optimization algorithms. It does take a longer time than other methods since numerical solving ODEs can be computational heavy (Tamburro 2019). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# References\n",
    "\n",
    "Tamburro, Alessio “Neural Network Back-Propagation Revisited with Ordinary Differential Equations.” Oct. 15, 2019 https://towardsdatascience.com/neural-network-back-propagation-revisited-892f42320d31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
