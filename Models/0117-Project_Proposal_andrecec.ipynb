{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  Optimization of Hyperparameters in a Convolution Neural Network </center>\n",
    "\n",
    "<center>By Cecilie Dura André </center>\n",
    "<center>andrecec@msu.edu</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"https://blog.ml.cmu.edu/wp-content/uploads/2018/12/heatmap.001-min.jpeg\" width=\"90%\">\n",
    "<p style=\"text-align: right;\">Image from: https://blog.ml.cmu.edu/2018/12/12/massively-parallel-hyperparameter-optimization/</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Overview\n",
    "\n",
    "\n",
    "In the last couple of years, there has been a major shift in training and using convolutional neural networks as a second opinion in medical detection and diagnostic. It has been proven in some medical cases that the neural networks outperform the detectors. Thus, they are thought to be able to work as a second opinion to minimize the time used on detecting and diagnosing a patient, while increasing sensitivity and specificity. \n",
    "\n",
    "Convolution neural networks can be trained by fitting the network of weights iteratively to a wished outcome by a known input. For each convolution neural networks, a few hyperparameters have to be picked. These parameters are often chosen before training and they are not changed under training. These parameters are picked based on experience and retraining the model a couple of times. This project will look into finding a method, either grid search or adaptive selection, to choose the hyperparameters while training the model. Thus, retraining the model is no longer necessary. The optimization algorithm will be used in a future convolution neural network trained on medical data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Program Description\n",
    "\n",
    "\n",
    "In this course, a \"dummy\" convolution neural network will be used together with the MNIST data set for training. The convolutional neural network is written by Lars Maaløe, Søren Kaae Sønderby, Casper Sønderby and updated by Toke Faurby to Pytorch and given on Github for the course 04256 Deep Learning at Danish Technical University. It can be used for free. The MNIST data set is also public and it consists of pictures of handwritten numbers and the belonging label. \n",
    "\n",
    "The goal of this project is to make an optimization algorithm, which will update hyperparameters to get the best outcome when the model is training. For visualization of how the optimization algorithm picks the hyperparameters, a heatmap (see the above picture) for specific epochs will be made. The heatmap will have a range of values for the axes and the heatmap will describe how accurate the model becomes for the chosen hyperparameters. A graph of the chosen values of hyperparameters for each epoch will also be made. For proof of concept, the network will be run e.g. 10 times with and without the optimization algorithm. The accuracy will be saved and inspected visually for significant differences between the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Project Goals and Timeline\n",
    "\n",
    "\n",
    "Project goals:\n",
    "\n",
    "- Find a method, grid search or adaptive selection algorithm, which can be used together with the given Pytorch optimization algorithm.\n",
    "- Implement the new optimization algorithm\n",
    "- Implement plots that visualize the function of the algorithm \n",
    "- Do a proof of comcept\n",
    "\n",
    "\n",
    "\n",
    "Timeline:\n",
    "- 1/13/2020 - Proposal Due\n",
    "\n",
    "        - 1/13/2020 - Have dummy example running (Convoluation Neural Network using MINST data set)\n",
    "        - 1/27/2020 - Search for methods (not bruch force) for hyperparameter optimization\n",
    "        - 1/27/2020 - Start coding\n",
    "        \n",
    "    - 1/31/2020 - Project git repository and structure is done\n",
    "    \n",
    "        - 1/31/2020 - Continue coding\n",
    "        \n",
    "    - 2/28/2020 - Coding standers and Unit Test integrated into project\n",
    "    \n",
    "        - 3/13/2020 - Finish up coding\n",
    "        - 3/20/2020 - Clean up code\n",
    "        \n",
    "    - 3/20/2020 - Code Draft (Code Review 1) \n",
    "    \n",
    "        - 4/01/2020 - Clean up code\n",
    "\n",
    "    - 4/03/2020 - Code Draft (Code Review 2) \n",
    "    \n",
    "        - 4/14/2020 - Clean up code\n",
    "\n",
    "- 4/15/2020 - Code Done\n",
    "- 4/20/2020 - Final Project and Presentation Due\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Anticipating Challenges  \n",
    "\n",
    "Optimization is already given as a function in the module Pytorch. Thus, implementing a secondary function, which can work together with the premade optimization algorithm or adapting the old optimization algorithm is going to be the biggest challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
