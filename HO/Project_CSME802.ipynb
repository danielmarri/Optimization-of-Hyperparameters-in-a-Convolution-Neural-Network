{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from itertools import accumulate\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.nn.functional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ ! -f mnist_cluttered_60x60_6distortions.npz ]; then wget -N https://www.dropbox.com/s/rvvo1vtjjrryr7e/mnist_cluttered_60x60_6distortions.npz; else echo \"mnist_cluttered_60x60_6distortions.npz already downloaded\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load data and minding to be smaller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "DIM = 60\n",
    "NUM_CLASSES = 10\n",
    "mnist_cluttered = \"mnist_cluttered_60x60_6distortions.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    \"\"\"\n",
    "    Load the npz-map and sort the data in a train, valid and test datset.\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        dict: A dictionary with the data sorted.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.load(mnist_cluttered)\n",
    "    X_train, y_train = data['x_train'], np.argmax(data['y_train'], axis=-1)\n",
    "    X_valid, y_valid = data['x_valid'], np.argmax(data['y_valid'], axis=-1)\n",
    "    X_test, y_test = data['x_test'], np.argmax(data['y_test'], axis=-1)\n",
    "\n",
    "    # reshape for convolutions\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, DIM, DIM))\n",
    "    X_valid = X_valid.reshape((X_valid.shape[0], 1, DIM, DIM))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, DIM, DIM))\n",
    "    \n",
    "    print(\"Train samples:\", X_train.shape)\n",
    "    print(\"Validation samples:\", X_valid.shape)\n",
    "    print(\"Test samples:\", X_test.shape)\n",
    "\n",
    "    return dict(\n",
    "        X_train=np.asarray(X_train, dtype='float32'),\n",
    "        y_train=y_train.astype('int32'),\n",
    "        X_valid=np.asarray(X_valid, dtype='float32'),\n",
    "        y_valid=y_valid.astype('int32'),\n",
    "        X_test=np.asarray(X_test, dtype='float32'),\n",
    "        y_test=y_test.astype('int32'),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        output_dim=10,)\n",
    "data = load_data()\n",
    "\n",
    "idx = 0\n",
    "canvas = np.zeros((DIM*NUM_CLASSES, NUM_CLASSES*DIM))\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        canvas[i*DIM:(i+1)*DIM, j*DIM:(j+1)*DIM] = data['X_train'][idx].reshape((DIM, DIM))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('Cluttered handwritten digits')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Defining the Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (localization): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(8, 10, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc_loc): Sequential(\n",
      "    (0): Linear(in_features=2250, out_features=32, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=32, out_features=6, bias=True)\n",
      "  )\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=800, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels = 1, input_height =60, input_width = 60, num_classes = 10 , num_zoom=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        self.num_classes = num_classes\n",
    "        self.num_zoom = num_zoom\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        # nn.Sequential http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\n",
    "        #   A sequential container. \n",
    "        #   Modules will be added to it in the order they are passed in the constructor.\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels,\n",
    "                      out_channels=8, \n",
    "                      kernel_size=7, \n",
    "                      padding=3),\n",
    "            nn.MaxPool2d(kernel_size=2, \n",
    "                         stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=8, \n",
    "                      out_channels=10, \n",
    "                      kernel_size=5, \n",
    "                      padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix that we use \n",
    "        # to make the bilinear interpolation for the spatial transformer\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(in_features=10 * input_height//4 * input_width//4, \n",
    "                      out_features=32,\n",
    "                      bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=32, \n",
    "                      out_features=3 * 2,\n",
    "                      bias=True)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        # see the article for a definition and explanation for this\n",
    "        self.fc_loc[2].weight.data.fill_(0)\n",
    "        self.fc_loc[2].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
    "        \n",
    "        # The classification network based on the transformed (cropped) image\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=5,\n",
    "                               padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=5,\n",
    "                               padding=2)\n",
    "        \n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        \n",
    "        # fully connected output layers\n",
    "        self.fc1_features = 32 * input_height//num_zoom//4 * input_width//num_zoom//4\n",
    "        self.fc1 = nn.Linear(in_features=self.fc1_features, \n",
    "                             out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50,\n",
    "                             out_features=num_classes)\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        \"\"\" Spatial Transformer Network \"\"\"\n",
    "        # creates distributed embeddings of the image with the location network.\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * self.input_height//4 * self.input_width//4)\n",
    "        # project from distributed embeddings to bilinear interpolation space\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        \n",
    "        # define the output size of the cropped tensor\n",
    "        # notice that we divide the height and width with the amount of zoom\n",
    "        output_size = torch.Size((x.size()[0],\n",
    "                                  x.size()[1], \n",
    "                                  x.size()[2]//self.num_zoom,\n",
    "                                  x.size()[3]//self.num_zoom))\n",
    "        # magic pytorch functions that are used for transformer networks\n",
    "        grid = F.affine_grid(theta, output_size) # http://pytorch.org/docs/master/nn.html#torch.nn.functional.affine_grid\n",
    "        x = F.grid_sample(x, grid) # http://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "        # save transformation\n",
    "        l_trans1 = Variable(x.data)\n",
    "\n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, self.fc1_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # note use of Functional.dropout, where training must be explicitly defined (default: False)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        # return output and batch of bilinear interpolated images\n",
    "        return F.log_softmax(x, dim=1), l_trans1\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if torch.cuda.is_available():\n",
    "    print('##converting network to cuda-enabled')\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([45, 10]), torch.Size([45, 1, 20, 20])]\n"
     ]
    }
   ],
   "source": [
    "# test forward pass on dummy data\n",
    "x = np.random.normal(0,1, (45, 1, 60, 60)).astype('float32')\n",
    "x = Variable(torch.from_numpy(x))\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "output = net(x)\n",
    "print([x.size() for x in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_BO(\n",
    "    net: torch.nn.Module,\n",
    "    Input: data['X_train'], \n",
    "    Label: data['y_train'],\n",
    "    parameters: Dict[str, float],\n",
    ") -> nn.Module:\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the network on provided data set to find the optimzed hyperparamter settings.\n",
    "\n",
    "    Args:\n",
    "        net: initialized neural network\n",
    "        Input: The image\n",
    "        Label: Th label to the respective image\n",
    "        parameters: dictionary containing parameters to be passed to the optimizer.\n",
    "            - lr: default (0.001)\n",
    "            - momentum: default (0.0)\n",
    "            - weight_decay: default (0.0)\n",
    "            - num_epochs: default (1)\n",
    "    Returns:\n",
    "        nn.Module: trained Network.\n",
    "    \"\"\"\n",
    "    # Define the data\n",
    "    X = data['X_train']\n",
    "    y = data['y_train']\n",
    "    \n",
    "    # Initilize network\n",
    "    net.train()\n",
    "    \n",
    "    # Define the hyperparameters\n",
    "    \n",
    "    criterion = nn.NLLLoss(reduction=\"sum\")\n",
    "    optimizer = optim.SGD(\n",
    "                net.parameters(),\n",
    "                lr=parameters.get(\"lr\", 0.001),\n",
    "                momentum=parameters.get(\"momentum\", 0.0),\n",
    "                weight_decay=parameters.get(\"weight_decay\", 0.0),\n",
    "                )\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "                optimizer,\n",
    "                step_size=int(parameters.get(\"step_size\", 30)),\n",
    "                gamma=parameters.get(\"gamma\", 1.0),  # default is no learning rate decay\n",
    "                )\n",
    "    num_epochs = parameters.get(\"num_epochs\", 1)\n",
    "    \n",
    "    # Get the number of samples and batches before starting trainging the network\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        for i in range(num_batches):\n",
    "            idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "            X_batch_tr = get_variable(Variable(torch.from_numpy(X[idx])))\n",
    "            y_batch_tr = get_variable(Variable(torch.from_numpy(y[idx]).long()))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = net(X_batch_tr)\n",
    "            batch_loss = criterion(output, y_batch_tr)\n",
    "        \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            \n",
    "    return net\n",
    "\n",
    "\n",
    "def eval_BO(\n",
    "    net: torch.nn.Module,\n",
    "    Input: data['X_valid'], \n",
    "    Label: data['y_valid'],\n",
    ") -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute classification accuracy on provided dataset to find the optimzed hyperparamter settings.\n",
    "\n",
    "    Args:\n",
    "        net: trained neural network\n",
    "        Input: The image\n",
    "        Label: Th label to the respective image\n",
    "    Returns:\n",
    "        float: classification accuracy\n",
    "    \"\"\"\n",
    "    # Define the data\n",
    "    X = data['X_valid']\n",
    "    y = data['y_valid']\n",
    "\n",
    "    # Pre-locating memory\n",
    "    pred_list = []\n",
    "    \n",
    "    # Get the number of samples and batches before testing the network\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i in range(num_batches):\n",
    "        \n",
    "            idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "            X_batch_val = get_variable(Variable(torch.from_numpy(X[idx])))\n",
    "            output, transformation = net(X_batch_val)\n",
    "            pred_list.append(get_numpy(output))\n",
    "        \n",
    "    # Calculating the accuracy\n",
    "    preds = np.concatenate(pred_list, axis=0)\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Hyperparameters(parameterization):\n",
    "    \"\"\"\n",
    "    Train and evaluate the network to find the best parameters \n",
    "    Args:\n",
    "        parameterization: The hyperparameters that should be evaluated\n",
    "    Returns:\n",
    "        float: classification accuracy\n",
    "    \"\"\"\n",
    "    net = Net()\n",
    "    net = train_BO(net=net, Input=data['X_train'], Label = data['y_train'], parameters=parameterization,)\n",
    "    \n",
    "    return eval_BO(net=net,Input=data['X_valid'], Label = data['y_valid'],)\n",
    "        \n",
    "\n",
    "    \n",
    "def evaluate_Hyperparameters_II(parameterization):\n",
    "    \"\"\"\n",
    "    Train and evaluate the network to find the best parameters \n",
    "    Args:\n",
    "        parameterization: The hyperparameters that should be evaluated\n",
    "    Returns:\n",
    "        float: classification accuracy\n",
    "    \"\"\"\n",
    "    net = train_BO(net=net, Input=data['X_train'], Label = data['y_train'], parameters=parameterization,)\n",
    "    \n",
    "    return eval_BO(net=net,Input=data['X_valid'], Label = data['y_valid'],)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 02-24 22:11:42] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 arms, GPEI for subsequent arms], generated 0 arm(s) so far). Iterations after 5 will take longer to generate due to model-fitting.\n",
      "[INFO 02-24 22:11:42] ax.service.managed_loop: Started full optimization with 20 steps.\n",
      "[INFO 02-24 22:11:42] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 02-24 22:13:26] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 02-24 22:15:11] ax.service.managed_loop: Running optimization trial 3...\n"
     ]
    }
   ],
   "source": [
    "# Saving the network so far, so it can be used when the optimization is done.\n",
    "\n",
    "NET = Net\n",
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "                {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-5, 0.1], \"log_scale\": False},\n",
    "                {\"name\": \"momentum\", \"type\": \"range\", \"bounds\": [0.2, 0.6]},\n",
    "                ],\n",
    "    evaluation_function=evaluate_Hyperparameters,\n",
    "    objective_name='accuracy',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving the results from the optimization\n",
    "BE = []\n",
    "ME = [] \n",
    "CO = []\n",
    "\n",
    "means, covariances = values\n",
    "\n",
    "BE.append(best_parameters)\n",
    "ME.append(means)\n",
    "CO.append(covariances)\n",
    "\n",
    "# Printing the results of the hyperparamter optimization\n",
    "print(best_parameters)\n",
    "print(means, covariances)\n",
    "\n",
    "# Findin the best hyperparameter for training the network\n",
    "data1 = experiment.fetch_data()\n",
    "df = data1.df\n",
    "best_arm_name = df.arm_name[df['mean'] == df['mean'].max()].values[0]\n",
    "best_arm = experiment.arms_by_name[best_arm_name]\n",
    "best_arm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(plot_contour(model=model, param_x='lr', param_y='momentum', metric_name='accuracy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_objectives = np.array([[trial.objective_mean*100 for trial in experiment.trials.values()]])\n",
    "best_objective_plot = optimization_trace_single_method(\n",
    "    y=np.maximum.accumulate(best_objectives, axis=1),\n",
    "    title=\"Model performance vs. # of iterations\",\n",
    "    ylabel=\"Classification Accuracy, %\",\n",
    ")\n",
    "render(best_objective_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_epoch(net: torch.nn.Module, Input: data['X_valid'], Label: data['y_valid'], parameters: Dict[str, float],) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    Train the network with the optimized hyperparamters.\n",
    "\n",
    "    Args:\n",
    "        Input: The image\n",
    "        Label: The label to the respective image\n",
    "        Param: Parameters the best paramters for training the network\n",
    "    Returns:\n",
    "        float: the mean cost and the classification accuracy\n",
    "    \"\"\"\n",
    "    # Define the data\n",
    "    X = data['X_valid']\n",
    "    y = data['y_valid']\n",
    "    \n",
    "    # Initilize network\n",
    "    net.train()\n",
    "    \n",
    "  # Define the hyperparameters    \n",
    "    optimizer = optim.SGD(\n",
    "                        net.parameters(),\n",
    "                        lr=parameters.get(\"lr\", 0.001),\n",
    "                        momentum=parameters.get(\"momentum\", 0.0),\n",
    "                        weight_decay=parameters.get(\"weight_decay\", 0.0),\n",
    "                        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Pre-locating memory\n",
    "    costs = []\n",
    "    correct = 0\n",
    "    \n",
    "    # Get the number of samples and batches before testing the network  \n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch_tr = get_variable(Variable(torch.from_numpy(X[idx])))\n",
    "        y_batch_tr = get_variable(Variable(torch.from_numpy(y[idx]).long()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = net(X_batch_tr)\n",
    "        batch_loss = criterion(output, y_batch_tr)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        costs.append(get_numpy(batch_loss))\n",
    "        preds = np.argmax(get_numpy(output), axis=-1)\n",
    "        correct += np.sum(get_numpy(y_batch_tr) == preds)\n",
    "    return np.mean(costs), correct / float(num_samples)\n",
    "\n",
    "def eval_epoch(net: torch.nn.Module, Input: data['X_valid'],  Label: data['y_valid'],) -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute classification accuracy on provided dataset to the optimized network.\n",
    "\n",
    "    Args:\n",
    "        Input: The image\n",
    "        Label: Th label to the respective image\n",
    "    Returns:\n",
    "        float: classification accuracy\n",
    "    \"\"\"\n",
    "    # Define the data\n",
    "    X = data['X_valid']\n",
    "    y = data['y_valid']\n",
    "    \n",
    "    # Pre-locating memory\n",
    "    pred_list = []\n",
    "    \n",
    "    # Get the number of samples and batches before testing the network\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    net.eval()\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch_val = get_variable(Variable(torch.from_numpy(X[idx])))\n",
    "        output, _ = net(X_batch_val)\n",
    "        pred_list.append(get_numpy(output))\n",
    "        \n",
    "    # Calculating the accuracy\n",
    "    preds = np.concatenate(pred_list, axis=0)\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the old network again with the new hyperparamter-setting\n",
    "Net = NET\n",
    "\n",
    "# Saving the results from the optimization\n",
    "valid_accs, train_accs, test_accs = [], [], []\n",
    "criterion = nn.NLLLoss(reduction=\"sum\")\n",
    "\n",
    "n = 0\n",
    "while n < NUM_EPOCHS:\n",
    "    n += 1\n",
    "    try:\n",
    "        print(\"Epoch %d:\" % n)\n",
    "        \n",
    "        train_cost, train_acc = train_epoch(net=net, Input=data['X_train'], Label=data['y_train'], parameters=best_arm.parameters,)\n",
    "        valid_acc = eval_epoch(net=net, Input=data['X_valid'], Label=data['y_valid'],)\n",
    "        test_acc = eval_epoch(net=net, Input=data['X_test'], Label=data['y_test'],)\n",
    "        valid_accs += [valid_acc]\n",
    "        test_accs += [test_acc]\n",
    "        train_accs += [train_acc]\n",
    "\n",
    "        print(\"train cost {0:.2}, train acc {1:.2}, val acc {2:.2}, test acc {3:.2}\".format(\n",
    "                train_cost, train_acc, valid_acc, test_acc))\n",
    "    \n",
    "        \"\"\" For every N new hyperparameters will be calculated for training the network\"\"\"\n",
    "        if n % 5 == 0:            \n",
    "            # Saving the network trained network so far, so it can be used when the optimization is done.\n",
    "            NET = net\n",
    "            \n",
    "            # Findinf the best hypperparameter again.\n",
    "            best_parameters, values, experiment, model = optimize(\n",
    "                    parameters=[\n",
    "                                {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-10, 0.1], \"log_scale\": True},\n",
    "                                {\"name\": \"momentum\", \"type\": \"range\", \"bounds\": [0.2, 0.6]},\n",
    "                                ],\n",
    "                    evaluation_function=evaluate_Hyperparameters_II,\n",
    "                    objective_name='accuracy',\n",
    "                    )\n",
    "            \n",
    "            \n",
    "            # Getting the best hyperparameters\n",
    "            data1 = experiment.fetch_data()\n",
    "            df = data1.df\n",
    "            best_arm_name = df.arm_name[df['mean'] == df['mean'].max()].values[0]\n",
    "            best_arm = experiment.arms_by_name[best_arm_name]\n",
    "\n",
    "            # Saving the results from the optimization\n",
    "            BE.append(best_parameters)\n",
    "            ME.append(means)\n",
    "            CO.append(CO)\n",
    "            \n",
    "            # Using the old network again with the new hyperparamter-setting\n",
    "            net = NET\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nKeyboardInterrupt')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "plt.plot(1 - np.array(train_accs), label='Training Error')\n",
    "plt.plot(1 - np.array(valid_accs), label='Validation Error')\n",
    "plt.plot(1 - np.array(test_accs), label='Test Error')\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('Error', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images, labels = data['X_test'], data['y_test']\n",
    "num_samples = images.shape[0]\n",
    "num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "pred_list = []\n",
    "transform_list = []\n",
    "net.eval()\n",
    "for i in range(1):\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch_val = get_variable(Variable(torch.from_numpy(images[idx])))\n",
    "        output, transformation = net(X_batch_val)\n",
    "        pred_list.append(get_numpy(output))\n",
    "\n",
    "labels = labels[idx]  \n",
    "preds = np.concatenate(pred_list, axis=0)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "predicted = preds\n",
    "\n",
    "for i in range(5):\n",
    "        idx_n = random.randint(0, len(images[idx]))\n",
    "        print('GroundTruth:', ' '.join('%5s' % labels[idx_n]), ',  Predicted:', ' '.join('%5s' % predicted[idx_n]))\n",
    "        image= X_batch_val[idx_n]\n",
    "        image = image[0,:,:]\n",
    "        imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "                        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
